{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##MULTI TASK DEEP LEARNING NEURAL NETWORK\n",
    "\n",
    "This contains a lot of the code from the other multitask file, but just what is required to run the multitask neural net with random splits multiple times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import torch\n",
    "from bayes_opt import BayesianOptimization\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import sqrt\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "#Load some of the data\n",
    "exp_data = pd.read_csv('../exp.tab', sep='\\t', index_col=0)\n",
    "cnv_data = pd.read_csv('../cnv.tab', sep='\\t', index_col=0)\n",
    "ydat = pd.read_csv('../labels.tab', sep='\\t', index_col=0)\n",
    "train_activity_data = pd.read_csv('../train_activity.tab', sep='\\t')\n",
    "test_activity_data = pd.read_csv('../test_activity.tab', sep ='\\t')\n",
    "\n",
    "#best ~1000 tasks\n",
    "top_tasks = pd.read_csv(\"../combined_stats.tab\", sep='\\t')\n",
    "tasks = top_tasks.iloc[:,0].values\n",
    "ydat_best = ydat.transpose()[tasks]\n",
    "ydat_best = ydat_best.transpose()\n",
    "\n",
    "#concatenate two data frames\n",
    "frames = [exp_data, cnv_data]\n",
    "\n",
    "xdatw = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Deep Learning Net Class\n",
    "\n",
    "class EssentialityNet:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.inputnum = xdatw.shape[0]\n",
    "        self.trainscores = []\n",
    "        self.testscoreslist = []\n",
    "        self.learning_rate = 0.00009\n",
    "        self.H = 100\n",
    "        self.n_iter = 300 #training iterations\n",
    "        self.minimum = 100000\n",
    "        self.stopcounter = 3\n",
    "        self.layernum = 1\n",
    "        self.layers = []\n",
    "                \n",
    "        #model\n",
    "        self.model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(self.inputnum, self.H),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(self.H, 1138),\n",
    "        )\n",
    "        \n",
    "        #set loss function and optimizer\n",
    "        self.loss = torch.nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "    #plot scores\n",
    "    def plot(self, trainscores, testscores):\n",
    "        x = np.arange(self.n_iter)\n",
    "        plt.plot(x, self.trainscores, label='Train')\n",
    "        plt.title('Training vs Test Accuracy')\n",
    "        plt.xlabel('NN Training Iterations')\n",
    "        plt.ylabel('Accuracy')\n",
    "    \n",
    "        plt.plot(np.asarray(x), np.asarray(testscores), label='Test') #plot\n",
    "        plt.legend()\n",
    "        \n",
    "    #sets the proper method\n",
    "    def setModel(self, Layernum, Neuronnum):  \n",
    "        \n",
    "        self.layernum = int(round(Layernum))\n",
    "        self.H = int(round(Neuronnum))\n",
    "        \n",
    "        #initial input layer\n",
    "        self.layers.append(torch.nn.Linear(self.inputnum, self.H))\n",
    "        \n",
    "        for n in range(self.layernum):\n",
    "            if n != 0:\n",
    "                self.layers.append(torch.nn.Linear(self.H, self.H))\n",
    "            self.layers.append(torch.nn.ReLU())\n",
    "            \n",
    "        self.layers.append(torch.nn.Linear(self.H, 1138))\n",
    "        \n",
    "        #set the method to whatever layers were chosen\n",
    "        self.model = torch.nn.Sequential(*self.layers)\n",
    "    \n",
    "    def setRegularization(self, L2Reg):\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay= L2Reg)\n",
    "\n",
    "    def fit(self, xtrain, ytrain, xtest, ytest):\n",
    "      \n",
    "        #convert to variables\n",
    "        xtrain_var = Variable(torch.FloatTensor(xtrain))\n",
    "        xtest_var = Variable(torch.FloatTensor(xtest))\n",
    "        ytrain_var = Variable(torch.FloatTensor(ytrain))\n",
    "        ytest_var = Variable(torch.FloatTensor(ytest))\n",
    "        \n",
    "        for t in range(self.n_iter):\n",
    "        \n",
    "            #calculate loss\n",
    "            ypred = self.model(xtrain_var)\n",
    "\n",
    "            diff = self.loss(ypred, ytrain_var)\n",
    "            self.trainscores.append(diff.data[0])\n",
    "            \n",
    "            #test performance\n",
    "            ypredtest = self.model(xtest_var)\n",
    "            difftest = self.loss(ypredtest, ytest_var)\n",
    "            \n",
    "            #find the best point\n",
    "            if t > 10 and self.minimum < difftest.data[0]:\n",
    "                self.stopcounter -= 1\n",
    "\n",
    "                if self.stopcounter == 0:\n",
    "                    self.n_iter = t\n",
    "                    self.trainscores.pop()\n",
    "                    break\n",
    "            elif t > 10 and self.stopcounter < 3:\n",
    "                self.stopcounter += 1\n",
    "            \n",
    "            self.minimum = difftest.data[0]\n",
    "            \n",
    "            self.testscoreslist.append(difftest.data[0])\n",
    "            \n",
    "            #zero gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            #backpropagate\n",
    "            diff.backward() \n",
    "            #update weights\n",
    "            self.optimizer.step() \n",
    "\n",
    "    # predict with the test data\n",
    "    def predict(self, X):\n",
    "        \n",
    "        X_var = Variable(torch.FloatTensor(X))\n",
    "        return self.model(X_var) \n",
    "    \n",
    "#other functions for running the nn\n",
    "\n",
    "def figureoutnetwork(layernum, neuronnum, l2reg):\n",
    "    n = EssentialityNet()\n",
    "    n.setModel(layernum, neuronnum)\n",
    "    n.setRegularization(l2reg)\n",
    "            \n",
    "    n.fit(xtrain_val, ytrain_val, xtest_val, ytest_val)\n",
    "    predictions = n.predict(xtest)\n",
    "#     return(calculateRMSE(predictions, ytest))\n",
    "    saveRMSE(predictions, ytest)\n",
    "\n",
    "def figureoutnetwork3(neuronnum, l2reg):\n",
    "    n = EssentialityNet()\n",
    "    n.setModel(3, neuronnum)\n",
    "    n.setRegularization(l2reg)\n",
    "            \n",
    "    n.fit(xtrain_val, ytrain_val, xtest_val, ytest_val)\n",
    "    predictions = n.predict(xtest)\n",
    "    return(calculateRMSE(predictions, ytest))\n",
    "    \n",
    "#calculate RMSE function\n",
    "def calculateRMSE(predicts, actuals):\n",
    "    mses = []  \n",
    "    multitaskrmses = []\n",
    "    preds = predicts.data.numpy()\n",
    "\n",
    "    for i in range(preds.shape[1]):\n",
    "        mses.append(((preds[:,i] - actuals[:,i])**2).mean())\n",
    "        multitaskrmses.append(sqrt(mses[i]))\n",
    "\n",
    "    print(len(multitaskrmses))       \n",
    "    return(np.mean(multitaskrmses))\n",
    "\n",
    "def saveRMSE(predicts, actuals):\n",
    "    mses = []  \n",
    "    multitaskrmses = []\n",
    "    preds = predicts.data.numpy()\n",
    "\n",
    "    for i in range(preds.shape[1]):\n",
    "        mses.append(((preds[:,i] - actuals[:,i])**2).mean())\n",
    "        multitaskrmses.append(sqrt(mses[i]))\n",
    "    \n",
    "    #open a file for saving rmses\n",
    "    rmses_file = open('rmses_' + str(fileno) + \".tab\", 'w')\n",
    "    \n",
    "    for item in multitaskrmses:\n",
    "          rmses_file.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for fileno in range(10):\n",
    "\n",
    "    #starting runs with random splits:\n",
    "    traininglabels = random.sample(range(0, 206), 142)\n",
    "    traininglabels.sort()\n",
    "    testinglabels = random.sample([x for x in range(206) if x not in traininglabels], 64)\n",
    "\n",
    "    #index the data with the proper labels\n",
    "    xtrain_not_norm = xdatw.iloc[:,traininglabels].transpose()\n",
    "    xtest_not_norm = xdatw.iloc[:,testinglabels].transpose()\n",
    "    ytrain = ydat_best.iloc[:,traininglabels].transpose().values\n",
    "    ytest = ydat_best.iloc[:,testinglabels].transpose().values\n",
    "\n",
    "    #normalize inputs\n",
    "    xtrain = preprocessing.scale(xtrain_not_norm)\n",
    "    xtest = preprocessing.scale(xtest_not_norm)\n",
    "\n",
    "    #create validation set\n",
    "    xtrain_val, xtest_val, ytrain_val, ytest_val = train_test_split(xtrain, ytrain, test_size=0.2, random_state=434)\n",
    "\n",
    "    #do not uncomment this if you do not want to retrain\n",
    "###     figureoutnetwork(3, 356, 0.012)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#open and concatenate the files\n",
    "myrmses0 = pd.read_csv('rmses_0.tab', header=None)\n",
    "myrmses1 = pd.read_csv('rmses_1.tab', header=None)\n",
    "myrmses2 = pd.read_csv('rmses_2.tab', header=None)\n",
    "myrmses3 = pd.read_csv('rmses_3.tab', header=None)\n",
    "myrmses4 = pd.read_csv('rmses_4.tab', header=None)\n",
    "myrmses5 = pd.read_csv('rmses_5.tab', header=None)\n",
    "myrmses6 = pd.read_csv('rmses_6.tab', header=None)\n",
    "myrmses7 = pd.read_csv('rmses_7.tab', header=None)\n",
    "myrmses8 = pd.read_csv('rmses_8.tab', header=None)\n",
    "myrmses9 = pd.read_csv('rmses_9.tab', header=None)\n",
    "\n",
    "rmses_total = pd.concat((myrmses0, myrmses1, myrmses2, myrmses3, myrmses4, myrmses5, myrmses6, myrmses7, myrmses8, myrmses9), axis = 1, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88287675711216296, 1.0072323031489039, 0.88177436041238644, 1.0179128871816361, 1.2920429061912651, 0.96828059613781647, 0.95408248180106447, 0.91696788445218791, 0.93730629258663445, 0.94564459877571938, 0.87798662368956282, 0.97437422099480153, 0.96246703151148516, 1.0499148275623145, 0.89884352748322738, 1.0277953660458068, 0.84998935939761622, 0.98402821806435647, 0.98327962483926734, 0.93125070333988624, 1.1821626352287034, 1.0559528873367359, 1.0408643899588463, 0.91281806485834172, 0.89937104196618678, 1.1826298346303161, 0.93185268794902942, 0.97546240541254403, 0.8877555569902984, 1.3192856490385858, 0.89252482057317972, 1.2738657392190968, 1.1958060788728204, 1.0299591267269723, 0.92852836916426029, 1.0829862683333944, 0.93469690007102779, 0.93638164879042451, 1.0192730115402679, 1.2281747963154404, 0.98793499545876851, 1.2323393352298346, 0.84173873323728954, 1.2007099068350804, 1.185635709180703, 0.88626445694765099, 1.1161589412036155, 0.87832503302547271, 0.92403245674303258, 0.87241255827211739, 0.90265900180003411, 0.97525175557660249, 0.88709739822561651, 1.3007950050436077, 1.3035278544598305, 1.0023959849182715, 1.0045796552523756, 0.93299221353091721, 0.94932101560841475, 1.0561190310346216, 1.0121353413712093, 1.0677616260860079, 0.88969143467973544, 1.3927835593998585, 0.9618309956882074, 0.92051854486003237, 0.89926214946831495, 0.8937359016427544, 0.8913952587980033, 0.94653397290493102, 1.0070411718198566, 0.8929014072121424, 0.87812862224419808, 1.0585642696788489, 0.94692448730918355, 0.92888570565924167, 0.95541162670128066, 0.89888206408488391, 1.0346359713675297, 1.2797886204675812, 0.88165163817373104, 1.0035635643637737, 1.1462002401083811, 0.9881891983846105, 1.0019535816340981, 0.93142339079132785, 0.85536701852034702, 0.96212299772950005, 0.95399852466574209, 0.92428606414550674, 0.91534297220333338, 1.4723774729702248, 0.93271612002583648, 0.86969821691246418, 1.2534413324015847, 1.0382827408229367, 0.89140957996788828, 0.86553062284355187, 0.88778176013886745, 1.5713580622571208, 0.97639492646012316, 0.92707028435104344, 0.92364351734556627, 0.89389007206087601, 0.8708132530489745, 0.89277139033542385, 1.1230585205127677, 0.89219515128570692, 0.89031139050128449, 0.89017036188429033, 0.89034268629053925, 0.89295645000011137, 0.92080439847109008, 0.88932362776388385, 0.89197230813398176, 0.89072426281697548, 1.0941958832254421, 1.004673972468938, 1.0114735886859176, 1.2663232259616983, 0.89232212629649754, 0.89789985704123043, 0.90502167931113975, 1.0394784875309169, 1.0501752090710235, 0.96205497209290858, 0.88722716237681687, 1.3178986925150113, 1.074456854911777, 1.1103681479791188, 1.1206013897045963, 1.0482768980281851, 0.98617995331347719, 0.96535881260652301, 1.1501770069595716, 0.90295069629927871, 0.91922826308969796, 1.1098107187077271, 1.0134864800154699, 1.0064684385228904, 0.89139120735449917, 0.94392234326238766, 1.0154677473313523, 0.94532865292044566, 0.83286918478350491, 1.3129174162673527, 0.92639669128532121, 0.96294464335196928, 0.94565017338117008, 1.0502835353459801, 1.0522075545532306, 0.96263675028550111, 1.02824020027662, 0.95481972089465117, 0.95729396465455796, 0.91557159743703154, 1.150308771613155, 0.94578573849294434, 0.96947905864139816, 0.89592127977204128, 1.5401318253966529, 0.89436135203249401, 0.84812642459357035, 1.2923455431228348, 0.88006368881026142, 0.99768262486311632, 1.0016568683008686, 1.0481451437371694, 0.99433421929444576, 1.6136107949152787, 1.2629284793457489, 1.0736499991670052, 1.0556018325523397, 1.0027388217495705, 1.1127926278441846, 0.98021296757805332, 0.98179810478779783, 1.0122820115991817, 1.0965942469113774, 0.89044682305467604, 1.0089103795423802, 1.1677939133606707, 0.96326955363659916, 0.86997417005919253, 1.1681505998774493, 0.89008306615548405, 1.5666603243308306, 1.3375360166561294, 1.2013105524535814, 1.0159256194772428, 0.92939306653215203, 1.1842413097828888, 1.0003036142519135, 1.3824345293590399, 0.94205512707649641, 1.1712058163678321, 0.94476110758087639, 1.144848093877455, 0.98083271671637162, 1.0804284274548936, 1.2046184202135823, 0.91044079175773862, 1.0089353855497101, 0.92976220042133839, 0.96350619631401746, 1.0555544605028622, 0.89323902044262482, 0.99686952602909185, 0.95556993542691893, 1.1538756044449117, 1.1935894715521456, 1.2836734991817453, 0.98143325113339297, 1.1652388668514992, 0.96161040711658274, 0.78691991389322458, 1.0053128506271891, 0.93714562023307335, 1.0831627742931567, 0.97801373594510432, 0.8976034739767037, 1.0675573964769707, 0.9386340910069082, 0.98856690398437874, 0.89149928109195942, 0.90405470172890701, 0.91732806916837861, 1.0091670231483612, 0.89111981461640188, 1.0424451692549606, 0.96297492988201727, 0.95950489483054768, 0.79363426999642206, 0.91972917164656987, 0.88996142474509354, 1.0347433238887214, 0.89172722729202891, 1.1658172767273522, 0.77623259646424381, 1.0223708232813176, 1.1525517234091562, 0.93011294767670827, 0.89132314057520146, 1.1481016714172654, 0.96903405769882711, 0.83786650070669144, 1.0940712478256605, 0.96532722734460263, 0.9414665107929322, 1.1959206036114263, 0.93479597906298628, 1.0573352218741729, 0.95091559272989401, 0.89954308070403521, 1.0281896726279942, 1.0577503494832565, 0.89183743732647491, 1.1648674800564882, 1.0565106837970903, 0.95673339173080085, 0.89042946877357898, 0.88441186686406303, 1.2678173326051514, 0.93786103161306011, 0.9547275758221494, 1.3563036833197861, 1.0167834494790795, 0.95823084658881841, 1.1442542773284892, 1.102498009225781, 0.93683103738204299, 1.1534929907264748, 1.1910909156910168, 0.90847122090371857, 0.92920357087913552, 0.94652643044220908, 0.89230174803030882, 0.96824896166879471, 0.89743569876883722, 1.1221098187318321, 0.89834290614895396, 1.085816109427771, 0.92613047683171801, 0.93316851075397267, 1.4152205710954207, 1.1847657807713499, 0.97314873150371706, 0.99059229256583203, 1.7693915739509556, 0.94831681720243799, 0.86945605196290665, 1.0400091011436645, 0.89086737917332548, 0.89383002670092393, 1.0168081248753629, 0.87215900201241858, 0.89841139393781921, 0.89312083867313807, 0.89035828440303688, 0.89364702161099796, 0.8923411713898256, 0.89451658425611824, 0.96145690413395291, 0.95894656324853644, 0.95984199747972487, 0.89125030570429009, 1.0799061548821083, 1.2638980267285127, 0.95344117411376161, 0.9313960740533147, 1.1933784708071069, 0.97353769073180563, 0.91091637392510982, 0.89901442598830084, 0.93956797515922708, 0.95870787300509119, 0.91805457251672462, 0.90912626269220864, 1.097993586149038, 1.2174508369298402, 0.89730071631029773, 0.89011750431462799, 0.98014928215070185, 0.86206978296637105, 1.0197553088727105, 0.85328981444940977, 1.0219823105550874, 1.1153827294386449, 0.92485537161850861, 0.94182545889167435, 0.88690615122098537, 0.88658915861474108, 0.97863838529302571, 1.0117994631268412, 0.9565148414878939, 0.9942941157169578, 1.0296204317365161, 1.1315826841684613, 0.89113594430538046, 1.0130540844564586, 1.3078814888237598, 0.86225399187364304, 0.89417318166720816, 1.0772587992713023, 0.92934635333386295, 0.97790124153532287, 0.97624977531103718, 0.88975103216851603, 1.282581674269037, 2.6104489301055609, 1.0866146357158197, 0.98447343965101641, 1.1609563778165721, 1.0489396850348318, 1.31378453084811, 0.88445508401753803, 0.89594732584546732, 0.90638089299481805, 0.99585742711521097, 0.98759895070138126, 1.1016397271368343, 0.97625795447365216, 1.3875477191597891, 0.97711294241791113, 1.0701309701349664, 0.96623632848802676, 0.94880474526812963, 0.921043529958256, 1.3968668196119931, 0.94055177469148232, 1.041517533458548, 1.0850852607017589, 0.8967460609183997, 0.91550925296996299, 0.88011772387321086, 0.90994209794166936, 0.95526079730823032, 0.91051629032870007, 0.90824822442251862, 0.90972429710262015, 0.91167021095164336, 0.90612617326142364, 0.90825184101279322, 0.98683043567664241, 1.0821100217033952, 0.87636072262735443, 1.1413693234751627, 2.4078881363286002, 0.99546918496654069, 1.098970922922236, 1.1348741930149131, 1.0975848577166376, 1.0852587411867982, 1.0939671275162819, 1.292676451767979, 0.87580495906445976, 1.1582513002592809, 0.88863210699363315, 2.0362516724456237, 0.93368888547300988, 0.97812496779353686, 1.1895776479333693, 0.93271128979141138, 0.94159510997643436, 0.95105768978392802, 0.893550492722469, 0.95409964843885542, 0.92023869446763373, 0.85130541028571671, 1.3452687716142884, 0.97149150660125938, 0.94748549285986738, 1.127020337314915, 0.9336404116333451, 0.89505225981564818, 1.0591307101150533, 0.91683537146419414, 1.0763826380344486, 0.89562587629761503, 0.92566659385018879, 0.84155878433146314, 1.6930982229110856, 1.1702965613943044, 1.2488905648396966, 1.167158427764208, 0.89613812425093098, 0.89515168430488889, 0.89008629008790996, 0.96295611061096609, 1.0090825195923192, 0.92676283812259863, 0.9923009424604754, 1.0639344174773924, 0.88275123509674547, 1.2968984085688759, 1.2881874902490733, 0.83217487649653865, 1.1530889665677542, 1.0964315748950892, 1.1374197328431073, 1.2108365628297468, 0.93039958596212835, 1.017412328808718, 1.2445210584084716, 0.98757599894019665, 0.79239649486807218, 0.93350280363676075, 0.91251392805038345, 0.88941661871063304, 0.97484238640549048, 0.97898036006101063, 1.1029433665538453, 1.025543971074629, 1.1265856540152843, 0.91650518243085877, 0.97268247389110518, 0.93087922507885901, 0.97431337040870147, 0.93958962441921867, 1.1035994119624231, 1.1390334758711069, 1.049925258447042, 0.97664115889707603, 0.99439450532121754, 0.96780548579110837, 0.89180615176233924, 0.90317758474304044, 0.88523078998363758, 1.1316449584930621, 0.92539886738837818, 0.96708052040028369, 1.051618546701651, 0.89499192917706449, 0.89049944235619127, 0.97576785469622906, 0.89328145563923189, 0.88960771716577258, 0.89108276609445214, 0.8929096089118671, 0.89240392118872336, 0.89150312464959813, 0.89541787346474389, 0.87544108231676798, 0.89059674400375, 0.88978403551894281, 0.89034757095384531, 0.89214222894589668, 0.89420437357567617, 0.8929382860450239, 0.89026626507757167, 0.89076666113114766, 0.88551276438392912, 0.88676514890315428, 0.88921218373032596, 0.89621462916310402, 0.89602130218272591, 0.89352375500008274, 0.89419930556119209, 0.93751927527554701, 0.88992810193231586, 0.89358830711024895, 0.89200899100946374, 0.89474847661612633, 0.88750687665036576, 0.88835433894894822, 0.88729862075785992, 1.0524514441631625, 0.88440934871823118, 0.89033282504781219, 1.1470939652200962, 0.88877556897155396, 0.89481848769033989, 1.0850865318103229, 0.981421855680777, 1.0257049672958849, 1.0039666270325809, 1.1487050120726725, 1.1495749597973002, 1.1490753140697179, 0.8937893960529486, 0.86371689885440939, 0.87853513343520917, 1.1161147130926046, 0.97282501852378667, 1.034078619593745, 0.99029705529878687, 0.90612595060622048, 0.85203769713670796, 1.1899008402947151, 1.0160182887706712, 0.97376096607317775, 1.0654558991573171, 1.0456188065765251, 0.9459785498045139, 0.9184503769654212, 0.97875722434138679, 1.0116531907796591, 1.0778706291520019, 0.9196076104533617, 1.1526804884200863, 0.90809386112096901, 1.0442954432312059, 1.151704735884578, 0.93896872059034775, 0.95870788489617131, 1.0274436359357275, 0.88694162640570862, 0.89387330412786348, 1.3289382357264909, 0.89266717841386556, 1.2182682667328506, 1.2629976039393616, 1.4321725966424197, 0.88807815576776206, 1.0467775314692431, 1.0197171256145079, 0.90643926716416046, 1.0527099100980957, 0.94733757963175624, 0.99051822234243692, 0.9006976228764868, 0.9048619393581907, 1.1382462077534954, 0.8908537890737771, 1.0046648319704494, 0.89711994650335991, 1.3093370117474201, 0.97182018560379857, 0.98676182111488564, 1.1078971525514678, 1.0059823425814229, 0.89920593838150487, 1.0674119725081006, 0.83765234525187027, 0.92461128919027991, 0.83669264464693927, 0.89336976232956933, 0.99338873265341232, 1.2555471440263322, 0.87165918027992217, 1.0075534528075372, 0.89751473700169326, 0.89188779355465275, 1.0063768938752731, 1.1008198903240956, 1.0430999979142459, 0.88192452502660679, 0.969068215250943, 1.0445848313622059, 0.91352071260062184, 0.93033031600520477, 1.3868828738762855, 1.34596553386818, 1.0861625405729995, 1.1202822154114589, 1.0182457838364971, 1.1078679896376351, 0.88765780588987475, 1.0101526990317684, 0.89284527008223724, 0.9801752054791425, 1.0241152404244087, 1.3535465986925801, 1.099095457172812, 1.1455581407326392, 1.0992030320451283, 0.95782704491119852, 0.97991353544313531, 0.99408468083200996, 1.0039027938433338, 1.0086743344380449, 0.81015469562318498, 1.1595487343171944, 1.0483105792185103, 0.90781420925057377, 0.97652968979074672, 1.0025866040411109, 0.92299129137524505, 0.90131679635062623, 0.92050567674465467, 1.4522200630616053, 0.9249264163525861, 1.1141955296894896, 1.0278493241728737, 1.0290857695436433, 0.99959793988033974, 0.9352630198997044, 0.9660213627712253, 0.8555510859142782, 0.85809513761179712, 0.93197485408636604, 1.0550572497942461, 1.4554416540273594, 1.1107762049357639, 1.1774713612322993, 1.1700684024736514, 1.2398630033251916, 1.0066876769617392, 1.0121638175352916, 1.0734936346384991, 0.97987497676787161, 0.82757514202622884, 1.0461149308553395, 1.0273023245225881, 0.8952356045410792, 0.94711908686333168, 0.94806100624717293, 0.81597457693854258, 0.9020500634093086, 0.97547507479552087, 0.94824178719399588, 0.95257499174307392, 0.9553850413785252, 0.92933395773618988, 0.81251445607511241, 0.99710505932278348, 0.83434170598821367, 1.4162898990245361, 1.0209241721330902, 0.93593853792976223, 1.3398605700379416, 0.99440614941489613, 1.0478331277455064, 1.2368896603278159, 0.92232251567790746, 0.97478699406857405, 0.8262247906219905, 0.89616761469660222, 0.91956494716231085, 1.0908981033157499, 0.87660471196215661, 0.96970821656238848, 1.2690327907019019, 1.0025495854936639, 0.97542321066286397, 1.5978155526123257, 0.8316501582625262, 1.104298507682866, 1.0555924647551964, 0.89428174888174805, 0.91924344695461468, 0.88493857986872315, 1.2302975377469791, 1.2019067001210713, 1.0679579496693825, 1.0702649286780221, 0.89887500885260896, 0.96584403315100786, 0.89159162740738052, 0.87602487938031026, 0.90679097520911756, 0.91943971292314264, 0.9122252163794794, 1.0418125967522751, 1.0701203731924336, 1.0882950884189524, 1.0660425611411379, 1.0989086432727178, 0.88934276478645768, 1.0012740252671428, 0.89202748898734385, 1.1538088430880773, 0.98039621889272177, 0.9923832288198412, 0.96208727365311675, 1.0763544507899838, 1.0480110396779694, 1.0849424165645685, 0.96690829091137398, 0.89285636691187487, 1.2635095434047241, 0.92814856613151608, 0.84496349969615747, 1.0344964223108257, 1.4801164334096852, 0.93485266315689197, 0.90819459056107976, 0.94823594979514336, 0.94688884641836801, 0.92238880359583475, 1.0062851437960028, 1.1689708008536592, 0.91818731601097314, 0.85036698033380342, 0.91114432989913419, 1.0386134904892081, 0.96887172713396841, 1.0977579514295579, 0.9722121146950371, 0.98876017778380798, 1.0284332154959885, 0.99839121697333832, 0.99323609971701621, 0.97867360193472008, 0.85652433071155687, 1.0214941271039695, 0.88853040480415224, 1.0076268228447514, 0.81729559446886557, 1.2016661522888479, 1.0612994022670774, 1.1559462538904879, 1.0415970010595277, 0.95750042389421353, 1.0609781161751981, 0.91656854641723551, 0.86356911733104269, 1.3182361826025104, 1.0170589180341274, 1.0517090641303906, 1.0632751014772743, 1.015059030853172, 0.83724824447891566, 1.0728599546893463, 0.97588444972237642, 0.97454650450994684, 1.2837873665463095, 1.0404195582796083, 0.99153263435355954, 1.1400055871679957, 0.96782393641198716, 1.072107065245081, 1.2505752360805245, 0.96488359316064309, 1.0522890684773816, 1.2311355340749743, 1.1324824798137447, 0.91917184585064349, 1.1086539397175126, 0.9757708793838914, 1.4542443787450114, 1.2022606490844889, 1.2431025891967977, 1.0900171813636983, 1.052878363802634, 1.0961052203372232, 1.1330665122330286, 0.84230018471842261, 1.0866056476151691, 0.89077023538866995, 0.91405587783583664, 1.1802675379711629, 0.98209440054652963, 0.88306339289862379, 1.2315027499177098, 0.8891044956574552, 0.87571053951266575, 1.1291414924034833, 0.86702643016792036, 1.0186511033100545, 0.89318308250960476, 0.91968070954397818, 0.85536062686889536, 1.0666831680623348, 0.94601957560282701, 0.89470639433407873, 0.88814556128859046, 1.2138265980734793, 1.0954417933423708, 1.0207181097334828, 1.074188437619753, 1.1411221012011201, 0.99024130900780705, 1.0630373637279247, 1.018396434190223, 0.91784699570749884, 1.1079596749044465, 1.050457767450736, 1.2815691725987044, 0.99594083022349655, 0.9198526922523913, 0.8512706701088304, 0.85503929786664679, 0.88248058887528846, 1.1488922881564265, 0.95706757979190316, 1.0820985578208615, 0.89422946203326403, 0.9246890141216294, 0.92484217107572864, 0.98376819608129684, 0.8920683495884264, 1.4833598655330342, 0.86263784554168077, 0.92262839407439334, 1.0587277311264258, 0.96103360502075286, 0.99768975577862107, 0.89040789499926709, 0.93304565655392258, 1.005946216851316, 1.1446816191567228, 1.1548788508038306, 1.4763395378397517, 0.89291193291410786, 1.0865869816614615, 1.3554748304364037, 0.9538502040082415, 0.98704508446133532, 0.97256173987661987, 0.98099160744023628, 1.0717276554403461, 1.2329203398628197, 0.83004048368750727, 1.0806384800404065, 1.2801910911566545, 1.1428617656488378, 1.2381702152762049, 1.0648469362136952, 1.0100589783525862, 0.81868894190683683, 1.0979540849414384, 1.0532274780661217, 1.0585192915732951, 0.93905738653556647, 1.2528496461454754, 0.91630072016471575, 1.2622514769437083, 0.87429689846969172, 0.95741910787110684, 0.88734695830153532, 1.1610127832939372, 1.1222441691903093, 0.84042806139427761, 0.89622267555548307, 1.2708810480541464, 0.88711881592512909, 0.97937415463007993, 1.0033559299108201, 0.9806730817306416, 0.95827691065610254, 0.96181282102496035, 1.3376843753208236, 0.88494450196399954, 0.94425085523376318, 1.0738810776745287, 0.93772488826823253, 0.87295578495124704, 1.1660255635287402, 0.89543806323654551, 1.0112085459636244, 0.95751031753812832, 1.1035194693280397, 0.82596868625004805, 1.0225993354668883, 0.92547041026175969, 1.2118772763399974, 0.91033023467288687, 0.89282411024430142, 1.0997642042254394, 0.90909277971597768, 1.1595624146424954, 0.9166679248748153, 0.96643035666969657, 0.84227669768544489, 0.88874566704136215, 0.9905592371406593, 0.91278081546277856, 1.1892941900571263, 1.2180952516945458, 0.84484458008519692, 1.0547500577923761, 1.2003892585482903, 0.97605432892858013, 0.97623377997755723, 0.83573095187475555, 1.3279414558304075, 1.0018663605249678, 1.0474008399269992, 1.4491744162266846, 0.94042775082351826, 1.0042623035867604, 0.88733431073503721, 1.0508053906549291, 0.98801802410679174, 1.1158499634910057, 1.0233896869470049, 0.97503273070725593, 0.94767958907453365, 0.96507269812381491, 0.97074170470379517, 0.88868056956661445, 1.4600551535202357, 0.97619446664471532, 1.3874632014805806, 1.3704540863436026, 1.3218412402604862, 1.0447525923017202, 1.0682014166788121, 1.0377229767190166, 1.1228454984320697, 1.217509400469823, 0.96585310029073024, 1.0213995079568932, 0.98189473581749809, 1.0445219610189214, 0.92943361772800037, 0.99904164177735111, 1.1324835337946852, 0.89148586084829984, 0.92844880819434716, 1.3134873908278242, 1.1149317357544948, 0.75388765179064388, 0.91842753469889149, 0.9768901965428689, 1.045965491534921, 0.88927878972046837, 1.0398705621085094, 1.0980272127469854, 1.0524893110146412, 0.94614139559796162, 1.0327378658851507, 0.97593481813976657, 0.85632930528181972, 0.96164802726121457, 0.95191745758841062, 0.86777736111097659, 1.0956105696692313, 0.89641635660753161, 0.94262999965236816, 1.0816168426318522, 1.0170554343527083, 0.92388234626518373, 0.91358667810277505, 1.069338472062846, 1.1811078743198211, 0.96941706797222016, 0.96331952703570867, 1.4295213681427723, 1.1508445778745562, 1.0374911985318123, 1.0184555320716435, 1.3516313194513165, 1.0676414729662951, 0.89035327250542573, 0.88298741651366908, 0.92161012420080579, 1.0785542771035384, 1.1667083344001543, 0.91431541784584136, 1.1132120426198324, 0.97382023925565275, 0.99639360447563574, 0.98196611764874808, 1.0274279118582259, 1.0263037183867576, 1.0084827009042665, 1.0785239041590555, 0.96681383174463631, 1.3208332003067449, 0.89287187222262399, 1.208781809395689, 0.92456221533885175, 0.89068178488340144, 0.86835143700371376, 1.0869857535431864, 1.0045286144208521, 1.0181924607821848, 0.86220195726630267, 0.87657219919663232, 1.2482532592657476, 0.93076358866590903, 0.8869384442934829, 0.9619334609077832, 0.94632525074802543, 0.99687948500122892, 0.99651698035949487, 0.97782687873615881, 0.99706803296784829, 0.90319617859800427, 0.96242301382660744, 0.94904053182029036, 0.89478930605832829, 1.1302937959675492, 0.95092586158221304, 1.3320001503176959, 0.95250487193233369, 1.2236538185424275, 0.99776247306832389, 1.1240646616211825, 0.9717108037188904, 0.97569654061116695, 1.2187560683035705, 0.96279701094148817, 0.91203825004171368, 1.0715611343636631, 0.88013874024348959, 0.87584946445554324, 0.88042990950402356, 0.88060309383434721, 0.8984667209841849, 0.95946248654891808, 0.95584918121771811, 0.97568973637630962, 0.88741423125734309, 0.8877101603994435, 0.85993501099896064, 0.89424525249191977, 0.94362448237345808, 0.93741957864632608, 0.89832683911740074, 1.0575758761722129, 0.88988883725598966, 1.0991798247561575, 0.98633242913887353, 0.92594168914718833, 0.9427961407760822, 1.1595952720328739, 1.0864247553020685, 0.92652652339505759, 0.89699665215388014, 0.95810657505855468, 0.94015750505246487, 1.0940791770057856, 1.0779026248703654, 0.97520735384611013, 0.88871110418111354, 0.90501069256650835, 0.82204531966913152, 0.87774137491342175, 0.93893153881314573, 1.2404605137261313, 1.1218876974385064, 1.0057564967298434, 1.266699145234075, 0.9795305212111044, 0.94229641697667488, 0.95277617735196363, 0.91418768317657462, 0.89400506827663029, 0.88803932880099867, 0.9356901999017978, 0.89553389736263234, 1.147859946754878, 0.88149853307434678, 1.1262603998354468, 1.1182522545767941, 1.0450702330098918, 1.154624059207892, 0.88682275603066874, 0.90583334296670548, 0.8787013275488883, 0.86852599108383077, 0.897000638989911, 0.98377205159176684, 0.93456696828550423, 0.89196632111670104, 0.9577721481674466, 0.96961544506020036, 0.97340633797894083, 0.89439901308190617, 0.89009826699731975, 1.1226970363428461, 0.97704580329378943, 0.95790701578145332, 1.0728111782195964, 0.91277545401933335, 1.0538257207389772, 1.0079768216994685, 0.88740365108193697, 1.1953617121624869, 0.88883038729886243, 0.89177688851453918, 0.97674363011471921, 0.90119936822086366, 0.9775852429914812, 0.91120878244662529, 0.89321718565537189, 0.88873144249925673, 0.88968955929687721, 0.81605680681068193, 0.8894857526092077, 0.99691167638067291, 0.91332532970359426, 0.88061732097001522, 0.93623410065389512, 0.89161614839965675, 1.0161414858517708, 1.0495046296842057, 0.89681641667966283, 1.0429784138425524, 0.89002300822486691, 0.91247024936350907, 1.0531425402133363, 1.0129325093220902, 0.88725281149865509, 0.92117851744473422, 1.0267533989524513, 0.81591606856759102, 0.90945830622965151, 0.94974667192549023, 1.0151410506053959, 1.370521149655576, 0.89675479512604528, 0.88882022421112017, 0.98174086076759937, 0.90525070450654721]\n"
     ]
    }
   ],
   "source": [
    "#get means across trials\n",
    "rmse_means = pd.DataFrame.mean(rmses_total, axis=1)\n",
    "print(list(rmse_means.values))\n",
    "\n",
    "#save to file\n",
    "mean_rmses_file = open('mean_rmses.tab', 'w')\n",
    "\n",
    "for item in list(rmse_means.values):\n",
    "    mean_rmses_file.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
