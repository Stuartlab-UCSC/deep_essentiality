{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##MULTI TASK DEEP LEARNING NEURAL NETWORK\n",
    "\n",
    "This contains a lot of the code from the other multitask file, but just what is required to run the multitask neural net with random splits multiple times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import torch\n",
    "from bayes_opt import BayesianOptimization\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import sqrt\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "#Load some of the data\n",
    "exp_data = pd.read_csv('../exp.tab', sep='\\t', index_col=0)\n",
    "cnv_data = pd.read_csv('../cnv.tab', sep='\\t', index_col=0)\n",
    "ydat = pd.read_csv('../labels.tab', sep='\\t', index_col=0)\n",
    "train_activity_data = pd.read_csv('../train_activity.tab', sep='\\t')\n",
    "test_activity_data = pd.read_csv('../test_activity.tab', sep ='\\t')\n",
    "\n",
    "#best ~1000 tasks\n",
    "top_tasks = pd.read_csv(\"../combined_stats.tab\", sep='\\t')\n",
    "tasks = top_tasks.iloc[:,0].values\n",
    "ydat_best = ydat.transpose()[tasks]\n",
    "ydat_best = ydat_best.transpose()\n",
    "\n",
    "#concatenate two data frames\n",
    "frames = [exp_data, cnv_data]\n",
    "\n",
    "xdatw = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep Learning Net Class\n",
    "\n",
    "class EssentialityNet:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.inputnum = xdatw.shape[0]\n",
    "        self.trainscores = []\n",
    "        self.testscoreslist = []\n",
    "        self.learning_rate = 0.00009\n",
    "        self.H = 100\n",
    "        self.n_iter = 300 #training iterations\n",
    "        self.minimum = 100000\n",
    "        self.stopcounter = 3\n",
    "        self.layernum = 1\n",
    "        self.layers = []\n",
    "                \n",
    "        #model\n",
    "        self.model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(self.inputnum, self.H),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(self.H, 1138),\n",
    "        )\n",
    "        \n",
    "        #set loss function and optimizer\n",
    "        self.loss = torch.nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "    #plot scores\n",
    "    def plot(self, trainscores, testscores):\n",
    "        x = np.arange(self.n_iter)\n",
    "        plt.plot(x, self.trainscores, label='Train')\n",
    "        plt.title('Training vs Test Accuracy')\n",
    "        plt.xlabel('NN Training Iterations')\n",
    "        plt.ylabel('Accuracy')\n",
    "    \n",
    "        plt.plot(np.asarray(x), np.asarray(testscores), label='Test') #plot\n",
    "        plt.legend()\n",
    "        \n",
    "    #sets the proper method\n",
    "    def setModel(self, Layernum, Neuronnum):  \n",
    "        \n",
    "        self.layernum = int(round(Layernum))\n",
    "        self.H = int(round(Neuronnum))\n",
    "        \n",
    "        #initial input layer\n",
    "        self.layers.append(torch.nn.Linear(self.inputnum, self.H))\n",
    "        \n",
    "        for n in range(self.layernum):\n",
    "            if n != 0:\n",
    "                self.layers.append(torch.nn.Linear(self.H, self.H))\n",
    "            self.layers.append(torch.nn.ReLU())\n",
    "            \n",
    "        self.layers.append(torch.nn.Linear(self.H, 1138))\n",
    "        \n",
    "        #set the method to whatever layers were chosen\n",
    "        self.model = torch.nn.Sequential(*self.layers)\n",
    "    \n",
    "    def setRegularization(self, L2Reg):\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay= L2Reg)\n",
    "\n",
    "    def fit(self, xtrain, ytrain, xtest, ytest):\n",
    "      \n",
    "        #convert to variables\n",
    "        xtrain_var = Variable(torch.FloatTensor(xtrain))\n",
    "        xtest_var = Variable(torch.FloatTensor(xtest))\n",
    "        ytrain_var = Variable(torch.FloatTensor(ytrain))\n",
    "        ytest_var = Variable(torch.FloatTensor(ytest))\n",
    "        \n",
    "        for t in range(self.n_iter):\n",
    "        \n",
    "            #calculate loss\n",
    "            ypred = self.model(xtrain_var)\n",
    "\n",
    "            diff = self.loss(ypred, ytrain_var)\n",
    "            self.trainscores.append(diff.data[0])\n",
    "            \n",
    "            #test performance\n",
    "            ypredtest = self.model(xtest_var)\n",
    "            difftest = self.loss(ypredtest, ytest_var)\n",
    "            \n",
    "            #find the best point\n",
    "            if t > 10 and self.minimum < difftest.data[0]:\n",
    "                self.stopcounter -= 1\n",
    "\n",
    "                if self.stopcounter == 0:\n",
    "                    self.n_iter = t\n",
    "                    self.trainscores.pop()\n",
    "                    break\n",
    "            elif t > 10 and self.stopcounter < 3:\n",
    "                self.stopcounter += 1\n",
    "            \n",
    "            self.minimum = difftest.data[0]\n",
    "            \n",
    "            self.testscoreslist.append(difftest.data[0])\n",
    "            \n",
    "            #zero gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            #backpropagate\n",
    "            diff.backward() \n",
    "            #update weights\n",
    "            self.optimizer.step() \n",
    "\n",
    "    # predict with the test data\n",
    "    def predict(self, X):\n",
    "        \n",
    "        X_var = Variable(torch.FloatTensor(X))\n",
    "        return self.model(X_var) \n",
    "    \n",
    "#other functions for running the nn\n",
    "\n",
    "def figureoutnetwork(layernum, neuronnum, l2reg):\n",
    "    n = EssentialityNet()\n",
    "    n.setModel(layernum, neuronnum)\n",
    "    n.setRegularization(l2reg)\n",
    "            \n",
    "    n.fit(xtrain_val, ytrain_val, xtest_val, ytest_val)\n",
    "    predictions = n.predict(xtest)\n",
    "#     return(calculateRMSE(predictions, ytest))\n",
    "    saveRMSE(predictions, ytest)\n",
    "\n",
    "def figureoutnetwork3(neuronnum, l2reg):\n",
    "    n = EssentialityNet()\n",
    "    n.setModel(3, neuronnum)\n",
    "    n.setRegularization(l2reg)\n",
    "            \n",
    "    n.fit(xtrain_val, ytrain_val, xtest_val, ytest_val)\n",
    "    predictions = n.predict(xtest)\n",
    "    return(calculateRMSE(predictions, ytest))\n",
    "    \n",
    "#calculate RMSE function\n",
    "def calculateRMSE(predicts, actuals):\n",
    "    mses = []  \n",
    "    multitaskrmses = []\n",
    "    preds = predicts.data.numpy()\n",
    "\n",
    "    for i in range(preds.shape[1]):\n",
    "        mses.append(((preds[:,i] - actuals[:,i])**2).mean())\n",
    "        multitaskrmses.append(sqrt(mses[i]))\n",
    "\n",
    "    print(len(multitaskrmses))       \n",
    "    return(np.mean(multitaskrmses))\n",
    "\n",
    "def saveRMSE(predicts, actuals):\n",
    "    mses = []  \n",
    "    multitaskrmses = []\n",
    "    preds = predicts.data.numpy()\n",
    "\n",
    "    for i in range(preds.shape[1]):\n",
    "        mses.append(((preds[:,i] - actuals[:,i])**2).mean())\n",
    "        multitaskrmses.append(sqrt(mses[i]))\n",
    "    \n",
    "    #open a file for saving rmses\n",
    "    rmses_file = open('rmses_' + str(fileno) + \".tab\", 'w')\n",
    "    \n",
    "    for item in multitaskrmses:\n",
    "          rmses_file.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fileno in range(10):\n",
    "\n",
    "    #starting runs with random splits:\n",
    "    traininglabels = random.sample(range(0, 206), 142)\n",
    "    traininglabels.sort()\n",
    "    testinglabels = random.sample([x for x in range(206) if x not in traininglabelnum], 64)\n",
    "\n",
    "    #index the data with the proper labels\n",
    "    xtrain_not_norm = xdatw.iloc[:,traininglabels].transpose()\n",
    "    xtest_not_norm = xdatw.iloc[:,testinglabels].transpose()\n",
    "    ytrain = ydat_best.iloc[:,traininglabels].transpose().values\n",
    "    ytest = ydat_best.iloc[:,testinglabels].transpose().values\n",
    "\n",
    "    #normalize inputs\n",
    "    xtrain = preprocessing.scale(xtrain_not_norm)\n",
    "    xtest = preprocessing.scale(xtest_not_norm)\n",
    "\n",
    "    #create validation set\n",
    "    xtrain_val, xtest_val, ytrain_val, ytest_val = train_test_split(xtrain, ytrain, test_size=0.2, random_state=434)\n",
    "\n",
    "    figureoutnetwork(3, 354, 0.013)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
