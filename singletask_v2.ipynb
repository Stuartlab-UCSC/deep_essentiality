{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['786O_KIDNEY', 'A1207_CENTRAL_NERVOUS_SYSTEM',\n",
       "       'A172_CENTRAL_NERVOUS_SYSTEM', 'A204_SOFT_TISSUE', 'A2058_SKIN',\n",
       "       'A549_LUNG', 'AGS_STOMACH', 'AML193_HAEMATOPOIETIC_AND_LYMPHOID_TISSUE',\n",
       "       'ASPC1_PANCREAS', 'BT20_BREAST',\n",
       "       ...\n",
       "       'TE9_OESOPHAGUS', 'THP1_HAEMATOPOIETIC_AND_LYMPHOID_TISSUE',\n",
       "       'TOV112D_OVARY', 'TYKNU_OVARY', 'U178_CENTRAL_NERVOUS_SYSTEM',\n",
       "       'U343_CENTRAL_NERVOUS_SYSTEM', 'U87MG_CENTRAL_NERVOUS_SYSTEM',\n",
       "       'UOK101_KIDNEY', 'VCAP_PROSTATE', 'ZR7530_BREAST'],\n",
       "      dtype='object', length=142)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Single task network to compare with Vlado\n",
    "\n",
    "\n",
    "import torch\n",
    "from bayes_opt import BayesianOptimization\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import sqrt\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "#Load some of the data\n",
    "exp_data = pd.read_csv('../exp.tab', sep='\\t', index_col=0)\n",
    "cnv_data = pd.read_csv('../cnv.tab', sep='\\t', index_col=0)\n",
    "ydat = pd.read_csv('../labels.tab', sep='\\t', index_col=0)\n",
    "train_activity_data = pd.read_csv('../train_activity.tab', sep='\\t')\n",
    "test_activity_data = pd.read_csv('../test_activity.tab', sep ='\\t')\n",
    "\n",
    "#labels\n",
    "traininglabels = train_activity_data.columns[1:]\n",
    "testinglabels = test_activity_data.columns[1:]\n",
    "\n",
    "#concatenate two data frames\n",
    "frames = [exp_data, cnv_data]\n",
    "\n",
    "xdatw = pd.concat(frames)\n",
    "traininglabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep Learning Net Class\n",
    "\n",
    "class EssentialityNet:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.inputnum = xdatw.shape[0]\n",
    "        self.trainscores = []\n",
    "        self.testscoreslist = []\n",
    "        self.learning_rate = 0.00009\n",
    "        self.H = 100\n",
    "        self.n_iter = 300 #training iterations\n",
    "        self.minimum = 100000\n",
    "        self.stopcounter = 3\n",
    "        self.layernum = 1\n",
    "        self.layers = []\n",
    "                \n",
    "        #model\n",
    "        self.model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(self.inputnum, self.H),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(self.H, 1),\n",
    "        )\n",
    "        \n",
    "        #set loss function and optimizer\n",
    "        self.loss = torch.nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "    #plot scores\n",
    "    def plot(self, trainscores, testscores):\n",
    "        x = np.arange(self.n_iter)\n",
    "        plt.plot(x, self.trainscores, label='Train')\n",
    "        plt.title('Training vs Test Accuracy')\n",
    "        plt.xlabel('NN Training Iterations')\n",
    "        plt.ylabel('Accuracy')\n",
    "    \n",
    "        plt.plot(np.asarray(x), np.asarray(testscores), label='Test') #plot\n",
    "        plt.legend()\n",
    "        \n",
    "    #sets the proper method\n",
    "    def setModel(self, Layernum, Neuronnum):  \n",
    "        \n",
    "        self.layernum = int(round(Layernum))\n",
    "        self.H = int(round(Neuronnum))\n",
    "        \n",
    "        #initial input layer\n",
    "        self.layers.append(torch.nn.Linear(self.inputnum, self.H))\n",
    "        \n",
    "        for n in range(self.layernum):\n",
    "            if n != 0:\n",
    "                self.layers.append(torch.nn.Linear(self.H, self.H))\n",
    "            self.layers.append(torch.nn.ReLU())\n",
    "            \n",
    "        self.layers.append(torch.nn.Linear(self.H, 1))\n",
    "        \n",
    "        #set the method to whatever layers were chosen\n",
    "        self.model = torch.nn.Sequential(*self.layers)\n",
    "    \n",
    "    def setRegularization(self, L2Reg):\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay= L2Reg)\n",
    "\n",
    "    def fit(self, xtrain, ytrain, xtest, ytest):\n",
    "      \n",
    "        #convert to variables\n",
    "        xtrain_var = Variable(torch.FloatTensor(xtrain))\n",
    "        xtest_var = Variable(torch.FloatTensor(xtest))\n",
    "        ytrain_var = Variable(torch.FloatTensor(ytrain))\n",
    "        ytest_var = Variable(torch.FloatTensor(ytest))\n",
    "        \n",
    "        for t in range(self.n_iter):\n",
    "        \n",
    "            #calculate loss\n",
    "            ypred = self.model(xtrain_var)\n",
    "\n",
    "            diff = self.loss(ypred, ytrain_var)\n",
    "            self.trainscores.append(diff.data[0])\n",
    "            \n",
    "            #test performance\n",
    "            ypredtest = self.model(xtest_var)\n",
    "            difftest = self.loss(ypredtest, ytest_var)\n",
    "            \n",
    "            #find the best point\n",
    "            if t > 10 and self.minimum < difftest.data[0]:\n",
    "                self.stopcounter -= 1\n",
    "\n",
    "                if self.stopcounter == 0:\n",
    "                    self.n_iter = t\n",
    "                    self.trainscores.pop()\n",
    "                    break\n",
    "            elif t > 10 and self.stopcounter < 3:\n",
    "                self.stopcounter += 1\n",
    "            \n",
    "            self.minimum = difftest.data[0]\n",
    "            \n",
    "            self.testscoreslist.append(difftest.data[0])\n",
    "            \n",
    "            #zero gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            #backpropagate\n",
    "            diff.backward() \n",
    "            #update weights\n",
    "            self.optimizer.step() \n",
    "\n",
    "    # predict with the test data\n",
    "    def predict(self, X):\n",
    "        \n",
    "        X_var = Variable(torch.FloatTensor(X))\n",
    "        predicts = self.model(X_var)\n",
    "        return predicts.data.numpy()\n",
    "    \n",
    "#other functions for running the nn\n",
    "\n",
    "def figureoutnetwork(layernum, neuronnum, l2reg):\n",
    "    n = EssentialityNet()\n",
    "    n.setModel(layernum, neuronnum)\n",
    "    n.setRegularization(l2reg)\n",
    "            \n",
    "    n.fit(xtrain_val, ytrain_val, xtest_val, ytest_val)\n",
    "    predictions = n.predict(xtest)\n",
    "    return(calculateRMSE_single(predictions, ytest))\n",
    "#     saveRMSE(predictions, ytest)\n",
    "\n",
    "def figureoutnetwork3(neuronnum, l2reg):\n",
    "    n = EssentialityNet()\n",
    "    n.setModel(3, neuronnum)\n",
    "    n.setRegularization(l2reg)\n",
    "            \n",
    "    n.fit(xtrain_val, ytrain_val, xtest_val, ytest_val)\n",
    "    predictions = n.predict(xtest)\n",
    "    return(calculateRMSE(predictions, ytest))\n",
    " \n",
    "#calculate RMSE function\n",
    "def calculateRMSE_single(predicts, actuals):\n",
    "#     mses = []  \n",
    "#     multitaskrmses = []\n",
    "#     preds = predicts.data.numpy()\n",
    "\n",
    "#     for i in range(preds.shape[1]):\n",
    "#         mses.append(((preds[:,i] - actuals[:,i])**2).mean())\n",
    "#         multitaskrmses.append(sqrt(mses[i]))\n",
    "\n",
    "#     print(len(multitaskrmses)) \n",
    "#     preds = predicts.data.numpy()\n",
    "\n",
    "    \n",
    "#     print(len(predicts))\n",
    "    predicts2 = [val for sublist in predicts for val in sublist]\n",
    "#     print(len(actuals))\n",
    "    print(predicts2)\n",
    "    print(actuals)\n",
    "\n",
    "    return sqrt(((predicts2 - actuals)**2).mean())\n",
    "\n",
    "#calculate RMSE function\n",
    "def calculateRMSE(predicts, actuals):\n",
    "    mses = []  \n",
    "    multitaskrmses = []\n",
    "    preds = predicts.data.numpy()\n",
    "\n",
    "    for i in range(preds.shape[1]):\n",
    "        mses.append(((preds[:,i] - actuals[:,i])**2).mean())\n",
    "        multitaskrmses.append(sqrt(mses[i]))\n",
    "\n",
    "    print(len(multitaskrmses))       \n",
    "    return(np.mean(multitaskrmses))\n",
    "\n",
    "def saveRMSE(predicts, actuals):\n",
    "    mses = []  \n",
    "    multitaskrmses = []\n",
    "    preds = predicts.data.numpy()\n",
    "\n",
    "    for i in range(preds.shape[1]):\n",
    "        mses.append(((preds[:,i] - actuals[:,i])**2).mean())\n",
    "        multitaskrmses.append(sqrt(mses[i]))\n",
    "    \n",
    "    #open a file for saving rmses\n",
    "    rmses_file = open('rmses_single_' + str(fileno) + \".tab\", 'w')\n",
    "    \n",
    "    for item in multitaskrmses:\n",
    "          rmses_file.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "AADAT_1_11010\n",
      "RMSE: \n",
      "0.9149543118242022\n",
      "1\n",
      "AASDHPPT_1_10110\n",
      "RMSE: \n",
      "1.076485023604067\n",
      "2\n",
      "ABCB1_1_11100\n",
      "RMSE: \n",
      "1.000051445413451\n",
      "3\n",
      "ABCB5_1_01010\n",
      "RMSE: \n",
      "1.213929665661624\n",
      "4\n",
      "ABCB7_1_10111\n",
      "RMSE: \n",
      "1.2723682081495553\n"
     ]
    }
   ],
   "source": [
    "#run for first 5 genes\n",
    "mselist = {}\n",
    "for i in range(5):\n",
    "    \n",
    "    #initialize\n",
    "    n = EssentialityNet()\n",
    "          \n",
    "    #best tasks\n",
    "    topTasks = pd.read_csv(\"../combined_stats.tab\", sep='\\t')\n",
    "    tasks = topTasks.iloc[:,0].values\n",
    "    \n",
    "    goodydat = ydat.transpose()[tasks]\n",
    "    goodydat = goodydat.transpose()\n",
    "    \n",
    "    #get name\n",
    "    name = goodydat.index[i]\n",
    "    \n",
    "    #get training and testing labels that vlado used\n",
    "    traininglabelsdat = pd.read_csv('../train_test_splits/train_test_splits/' + name + '_train_idx', header = None)\n",
    "    testinglabelsdat = pd.read_csv('../train_test_splits/train_test_splits/' + name + '_test_idx', header = None)\n",
    "    \n",
    "    #convert them to lists\n",
    "    traininglabels = traininglabelsdat.iloc[:,0].values\n",
    "    testinglabels = testinglabelsdat.iloc[:,0].values\n",
    "\n",
    "    #index the data \n",
    "    xtraindat = xdatw[traininglabels].transpose()\n",
    "    xtestdat = xdatw[testinglabels].transpose()\n",
    "    ytraindat = goodydat[traininglabels].transpose()\n",
    "    ytestdat = goodydat[testinglabels].transpose()\n",
    "    \n",
    "    #normalize inputs\n",
    "    xtrain = preprocessing.scale(xtraindat)\n",
    "    xtest = preprocessing.scale(xtestdat)\n",
    "\n",
    "    #get column (task)\n",
    "    ytrain = np.array(ytraindat.iloc[:,i])  \n",
    "    ytest = np.array(ytestdat.iloc[:,i]) \n",
    "    \n",
    "#     print(xtrain)\n",
    "#     print(ytrain)\n",
    "#     print(xtrain_val)\n",
    "    \n",
    "    #split for validation\n",
    "    xtrain_val, xtest_val, ytrain_val, ytest_val = train_test_split(xtrain, ytrain, test_size=0.2, random_state=434)\n",
    "    \n",
    "    print(i)\n",
    "    print(name)\n",
    "\n",
    "    #fit and predict\n",
    "    print(\"RMSE: \")\n",
    "    print(figureoutnetwork(3, 356, 0.012))\n",
    "    #3 356\n",
    "#     torch.save(n, \"/home/user/Documents/stuart_research/data/data/exp_cnv_activity/models/\" + name + \".txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
